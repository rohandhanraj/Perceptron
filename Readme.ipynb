{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2ff2c1",
   "metadata": {},
   "source": [
    "# 1. Perceptron\n",
    "**Single Layer Perceptron(one Neuron)**  \n",
    "A Perceptron is a simple model of a biological neuron that classifies the label of the input data based on various activation functions:\n",
    "1. Binary Step Function: \n",
    "\\begin{equation}\n",
    "  f(x)=\\begin{cases}\n",
    "    0, & \\text{if $x<0$}.\\\\\n",
    "    1, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "[Binary Step Function](Activation_Function_Plots\\step.png)\n",
    "\n",
    "2. Signum Function:\n",
    "\\begin{equation}\n",
    "  f(x)=\\begin{cases}\n",
    "    -1, & \\text{if $x<0$}.\\\\\n",
    "    0, & \\text{if $x=0$>}\\\\\n",
    "    1, & \\text{if $x>0$}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "[Signum Function](Activation_Function_Plots\\signum.png)\n",
    "\n",
    "3. Linear Activation Function:\n",
    "$$f(x) = x$$\n",
    "[Linear Activation Function](Activation_Function_Plots\\linear.png)\n",
    "\n",
    "4. Sigmoid / Logistic Activation Function:\n",
    "$$f(x) = \\frac{1}{1+e^{-x}}$$\n",
    "[Sigmoid / Logistic Activation Function](Activation_Function_Plots\\sigmoid.png)\n",
    "\n",
    "5. Tanh Function (Hyperbolic Tangent):\n",
    "$$f(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n",
    "![Tanh Function (Hyperbolic Tangent)](Activation_Function_Plots\\tanh.png)\n",
    "\n",
    "6. ReLU Function:\n",
    "$$f(x) = max(0, x)$$\n",
    "![ReLU Function](Activation_Function_Plots\\ReLU.png)\n",
    "\n",
    "7. Exponential Linear Units (ELUs) Function:\n",
    "\\begin{equation}\n",
    "  f(x)=\\begin{cases}\n",
    "    x, & \\text{if $x\\geqslant0$}.\\\\\n",
    "    \\alpha(e^{x} - 1), & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "![Exponential Linear Units (ELUs) Function](Activation_Function_Plots\\ELU.png)\n",
    "\n",
    "8. Swish:\n",
    "$$f(x) = \\frac{x}{1 + e^{-x}}$$\n",
    "![Swish](Activation_Function_Plots\\swish.png)\n",
    "\n",
    "9. Gaussian Error Linear Unit (GELU):\n",
    "$$f(x) = 0.5x(1 + tanh[\\sqrt{2/\\pi}(x + 0.044715x^3)])$$\n",
    "![Gaussian Error Linear Unit (GELU)](Activation_Function_Plots\\GELU.png)\n",
    "\n",
    "\n",
    "```python\n",
    "fn_list = ['step', 'signum', 'linear', 'relu', 'sigmoid', 'tanh', 'elu', 'gelu', 'swish']\n",
    "```\n",
    "\n",
    "## 1.1. How to use this\n",
    "```python\n",
    "from Perceptron.perceptron import Perceptron\n",
    "from Perceptron.utils import prepare_data, save_plot, save_model\n",
    "\n",
    "# get the data, convert it into a DataFrame and then use below commands\n",
    "X, y = prepare_data(df)\n",
    "\n",
    "model = Perceptron(eta = eta, epochs = epochs)\n",
    "model.fit(X, y, fn, alpha=None) # alpha ranges between 0 to 1 if and only if ELU activation function is applied else alpha value remains None for other activation functions\n",
    "\n",
    "Total_Error = model.total_loss()\n",
    "\n",
    "save_model(model, filename = filename)\n",
    "\n",
    "save_plot(df, plotFilename, model)\n",
    "```\n",
    "\n",
    "## 1.2. Reference\n",
    "[Python Package Publishing Docs](https://packaging.python.org/tutorials/packaging-projects/)\n",
    "\n",
    "[GitHub Actions CICD Docs](https://docs.github.com/en/actions/guides/building-and-testing-python#publishing-to-package-registries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
